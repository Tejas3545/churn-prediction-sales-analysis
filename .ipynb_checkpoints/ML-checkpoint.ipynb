{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwKQCoF0Xf_I"
      },
      "outputs": [],
      "source": [
        "# Customer Churn Prediction, Sales Analysis & AI Applications\n",
        "\n",
        "### Internship Project – Data Science\n",
        "This project analyzes customer behavior, predicts churn probability, and explores applications of ML & AI such as sales trend forecasting and conversational chatbots.\n",
        "It combines:\n",
        "- Supervised & Unsupervised ML\n",
        "- Regression & Classification\n",
        "- Deep Learning\n",
        "- NLP Chatbot concepts\n",
        "- Bayesian Probability applications\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "460YOt1wXep5"
      },
      "outputs": [],
      "source": [
        "# Basic Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score\n",
        "\n",
        "# Clustering\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Deep Learning\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulated Dataset for demonstration (replace with real data)\n",
        "data = {\n",
        "    'CustomerID': range(1, 21),\n",
        "    'Gender': ['Male','Female']*10,\n",
        "    'Age': np.random.randint(18, 60, 20),\n",
        "    'Tenure': np.random.randint(1, 24, 20),\n",
        "    'Balance': np.random.randint(1000, 10000, 20),\n",
        "    'NumOfProducts': np.random.randint(1, 4, 20),\n",
        "    'HasCrCard': np.random.randint(0, 2, 20),\n",
        "    'IsActiveMember': np.random.randint(0, 2, 20),\n",
        "    'EstimatedSalary': np.random.randint(30000, 100000, 20),\n",
        "    'Exited': np.random.randint(0, 2, 20)  # Target variable\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Encode categorical variables\n",
        "le = LabelEncoder()\n",
        "df['Gender'] = le.fit_transform(df['Gender'])  # Male=1, Female=0\n",
        "\n",
        "# Features and Target\n",
        "X = df.drop(['CustomerID','Exited'], axis=1)\n",
        "y = df['Exited']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lr = LogisticRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "y_pred_lr = lr.predict(X_test)\n",
        "\n",
        "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred_lr))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_lr))\n",
        "sns.heatmap(confusion_matrix(y_test, y_pred_lr), annot=True, cmap=\"Blues\", fmt=\"d\")\n",
        "plt.title(\"Confusion Matrix - Logistic Regression\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "\n",
        "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_rf))\n",
        "sns.heatmap(confusion_matrix(y_test, y_pred_rf), annot=True, cmap=\"Greens\", fmt=\"d\")\n",
        "plt.title(\"Confusion Matrix - Random Forest\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "clusters = kmeans.fit_predict(X)\n",
        "\n",
        "df['Cluster'] = clusters\n",
        "sns.scatterplot(x=df['Age'], y=df['Balance'], hue=df['Cluster'], palette='Set2')\n",
        "plt.title(\"Customer Segmentation using K-Means\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(16, activation='relu', input_dim=X_train.shape[1]))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(X_train, y_train, epochs=20, batch_size=4, validation_split=0.2, verbose=0)\n",
        "\n",
        "loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"ANN Accuracy:\", round(acc,3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fake transactions dataset for sales trend\n",
        "transactions = pd.DataFrame({\n",
        "    'date': pd.date_range(start=\"2023-01-01\", periods=100, freq=\"D\"),\n",
        "    'amount': np.random.randint(100, 1000, 100)\n",
        "})\n",
        "transactions['month'] = transactions['date'].dt.to_period('M')\n",
        "monthly_sales = transactions.groupby('month')['amount'].sum()\n",
        "\n",
        "monthly_sales.plot(kind='line', marker='o')\n",
        "plt.title(\"Monthly Sales Trend\")\n",
        "plt.xlabel(\"Month\")\n",
        "plt.ylabel(\"Total Sales\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Spam detection with Naive Bayes\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "texts = [\"Win a free iPhone\", \"Meeting tomorrow at 10\", \"Exclusive offer just for you\", \"Let’s have lunch\"]\n",
        "labels = [1, 0, 1, 0]  # 1=spam, 0=not spam\n",
        "\n",
        "cv = CountVectorizer()\n",
        "X_text = cv.fit_transform(texts)\n",
        "\n",
        "nb = MultinomialNB()\n",
        "nb.fit(X_text, labels)\n",
        "\n",
        "print(\"Predicted Spam Labels:\", nb.predict(X_text))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def simple_chatbot(user_input):\n",
        "    if \"hello\" in user_input.lower():\n",
        "        return \"Hi there! How can I help you?\"\n",
        "    elif \"price\" in user_input.lower():\n",
        "        return \"Our products range from $10 to $500.\"\n",
        "    elif \"bye\" in user_input.lower():\n",
        "        return \"Goodbye! Have a nice day.\"\n",
        "    else:\n",
        "        return \"I'm not sure about that, but I'm learning!\"\n",
        "\n",
        "# Test chatbot\n",
        "print(simple_chatbot(\"Hello\"))\n",
        "print(simple_chatbot(\"Tell me the price\"))\n",
        "print(simple_chatbot(\"bye\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Conclusion\n",
        "- Logistic Regression & Random Forest achieved good accuracy for churn prediction.  \n",
        "# - K-Means clustering revealed 3 customer segments.  \n",
        "# - ANN showed potential for deep learning-based churn detection.  \n",
        "# - Sales trends were analyzed to identify seasonal effects.  \n",
        "# - Bayes Theorem applied via Naive Bayes for spam classification.  \n",
        "# - A simple chatbot demonstrated AI-powered customer support possibilities.  \n",
        "\n",
        "# **Future Enhancements:**  \n",
        "# - Use larger real-world datasets.  \n",
        "# - Deploy churn model & chatbot in a web app.  \n",
        "# - Add real-time analytics dashboard.  \n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
